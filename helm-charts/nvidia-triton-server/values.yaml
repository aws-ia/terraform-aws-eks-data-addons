# Number of Triton server replicas for the deployment
replicaCount: 1

image:
  repository: nvcr.io/nvidia/tritonserver
  tag: "22.02-py3" # for vLLM look for tags like: 24.02-vllm-python-py3
  pullPolicy: IfNotPresent

# You can use local path like /models or s3://bucket-name/models
modelRepositoryPath: /models

# Service Configuration (Exposing Triton endpoints)
service:
  type: ClusterIP
  ports:
    http: 8000
    grpc: 8001
    metrics: 8002

  metricsPort: 8080

# Service account settings (Refer to Kubernetes documentation for details)
serviceAccount:
  create: false
  annotations: {}
  labels: {}
  name: ""

# Ingress Settings (Configure external access to Triton)
ingress:
  enabled: true
  className: nginx
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # nginx.ingress.kubernetes.io/use-regex: "true"
    # nginx.ingress.kubernetes.io/rewrite-target: "/$1"
    # OR
    # kubernetes.io/ingress.class: alb
    # alb.ingress.kubernetes.io/scheme: internet-facing
    # alb.ingress.kubernetes.io/target-type: ip
    # alb.ingress.kubernetes.io/success-codes: "200-299"
    # alb.ingress.kubernetes.io/healthcheck-path: "/v1/health/ready"
    # alb.ingress.kubernetes.io/healthcheck-port: "8080"
  hosts:
    - host: "example.com"
      paths:
        - path: /
          pathType: Prefix
          service:
            name:
            port:
              number: 8000
        # - path: /serve/(.*)
        #   pathType: ImplementationSpecific
        #   service:
        #     name:
        #     port:
        #       number: 8265
  tls: []
    # - hosts:
    #     - "example.com"
    #   secretName: "example-tls"

selectorLabels:
  app: triton-inference-server

podSecurityContext:
  runAsGroup: 1000
  runAsUser: 1000
  fsGroup: 1000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# Environment variables for Triton containers
environment:
  - name: "LD_PRELOAD"
    value: ""
  - name: "TRANSFORMERS_CACHE"
    value: "/home/triton-server/.cache"
  - name: "shm-size"
    value: "5g"
  - name: "NCCL_IGNORE_DISABLED_P2P"
    value: "1"
  # - name: "model_name"
  #   value: "meta-llama/Llama-2-7b-chat-hf"

# Secret environment variables to authenticate with Hugging Face to load models
secretEnvironment:
  - name: "HUGGING_FACE_TOKEN"
    secretName: "huggingface" # Name of the secret
    key: "HF_TOKEN"           # Key within the secret

resources:
  requests:  # Minimum resource requests for each Triton pod
    cpu: "100m"
    memory: "512Mi"
    nvidia.com/gpu: 1
  limits:  # Maximum resource limits
    cpu: "500m"
    memory: "2Gi"
    nvidia.com/gpu: 1

# Horizontal Pod Autoscaler (HPA) configuration
hpa:
  enabled: true
  minReplicas: 1
  maxReplicas: 5

# Advanced Configuration (If needed)
nodeSelector: {} # Schedule pods on specific nodes
tolerations: []  # Allow pods to be scheduled on nodes with 'taints'
affinity: {}     # Influence pod scheduling based on node or pod labels
